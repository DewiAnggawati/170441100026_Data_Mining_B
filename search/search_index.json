{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Nama : Dewi Anggawati NIM : 170441100026 Prodi : Sistem Informasi, Fakultas Teknik, Universitas Trunojoyo Madura Hi, Saya Dewi ! \u00b6 Saya seorang mahasiswi dari Universitas Trunojoyo Madura, Indonesia angkatan 2017 dan sedang menempuh pendidikan S1 Sistem Informasi. Yang mempelajari tentang pemrograman, business, analisis, sistem dan data. Dan saat ini saya sedang membuat tugas Data Mining salah satu matakuliah yang ada di jurusan saya.","title":"Beranda"},{"location":"#hi-saya-dewi","text":"Saya seorang mahasiswi dari Universitas Trunojoyo Madura, Indonesia angkatan 2017 dan sedang menempuh pendidikan S1 Sistem Informasi. Yang mempelajari tentang pemrograman, business, analisis, sistem dan data. Dan saat ini saya sedang membuat tugas Data Mining salah satu matakuliah yang ada di jurusan saya.","title":"Hi, Saya Dewi !"},{"location":"DECISION TREE  POHON KEPUTUSAN/","text":"DECISION TREE / POHON KEPUTUSAN \u00b6 Pengertian Pohon Keputusan \u00b6 Salah satu metode data mining yang umum digunakan adalah pohon keputusan . Metode pohon keputusan mengubah fakta yang sangat besar menjadi pohon keputusan yang merepresentasikan rule. Pohon keputusan adalah salah satu metode klasifikasi yang paling popular karena mudah untuk diinterpretasi oleh manusia. Konsep dari pohon keputusan adalah mengubah data menjadi pohon keputusan dan aturan-aturan keputusan. Konsep Pohon Keputusan Data dalam pohon keputusan biasanya dinyatakan dalam bentuk tabel dengan atribut dan record. Atribut menyatakan suatu parameter yang dibuat sebagai kriteria dalam pembentukan tree. Konsep Data dalam Pohon Keputusan Proses pada pohon keputusan adalah mengubah bentuk data (tabel) menjadi model pohon, mengubah model pohon menjadi rule, dan menyederhanakan rule. Manfaat Pohon Keputusan \u00b6 Manfaat utama dari penggunaan pohon keputusan adalah kemampuannya untuk mem- break down proses pengambilan keputusan yang kompleks menjadi lebih simpel sehingga pengambil keputusan akan lebih menginterpretasikan solusi dari permasalahan. Pohon Keputusan juga berguna untuk mengeksplorasi data, menemukan hubungan tersembunyi antara sejumlah calon variabel input dengan sebuah variabel target. Pohon keputusan memadukan antara eksplorasi data dan pemodelan, sehingga sangat bagus sebagai langkah awal dalam proses pemodelan bahkan ketika dijadikan sebagai model akhir dari beberapa teknik lain. Sering terjadi tawar menawar antara keakuratan model dengan transparansi model. Dalam beberapa aplikasi, akurasi dari sebuah klasifikasi atau prediksi adalah satu-satunya hal yang ditonjolkan, misalnya sebuah perusahaan direct mail membuat sebuah model yang akurat untuk memprediksi anggota mana yang berpotensi untuk merespon permintaan, tanpa memperhatikan bagaimana atau mengapa model tersebut bekerja. Konsep Dasar Pohon Keputusan Bagian awal dari pohon keputusan ini adalah titik akar (root), sedangkan setiap cabang dari pohon keputusan merupakan pembagian berdasarkan hasil uji, dan titik akhir (leaf) merupakan pembagian kelas yang dihasilkan. Pohon keputusan mempunyai 3 tipe simpul yaitu: Simpul akar, dimana tidak memiliki cabang yang masuk dan memiliki cabang lebih dari satu, terkadang tidak memiliki cabang sama sekali. Simpul ini biasanya berupa atribut yang paling memiliki pengaruh terbesar pada suatu kelas tertentu. Simpul internal, dimana hanya memiliki 1 cabang yang masuk, dan memiliki lebih dari 1 cabang yang keluar. Simpul daun, atau simpul akhir dimana hanya memiliki 1 cabang yang masuk, dan tidak memiliki cabang sama sekali dan menandai bahwa simpul tersebut merupakan label kelas. Tahap awal dilakukan pengujian simpul akar, jika pada pengujian simpul akar menghasilkan sesuatu maka proses pengujian juga dilakukan pada setiap cabang berdasarkan hasil dari pengujian. Hal ini berlaku juga untuk simpul internal dimana suatu kondisi pengujian baru akan diterapkan pada simpul daun. Pada umumnya proses dari sistem pohon keputusan adalah mengadopsi strategi pencarian top-down untuk solusi ruang pencariannya. Pada proses mengklasifikasikan sampel yang tidak diketahui, nilai atribut akan diuji pada pohon keputusan dengan cara melacak jalur dari titik akar sampai titik akhir, kemudian akan diprediksikan kelas yang ditempati sampel baru tersebut. Kelebihan Pohon Keputusan \u00b6 Kelebihan dari metode pohon keputusan adalah: Daerah pengambilan keputusan yang sebelumnya kompleks dan sangat global, dapat diubah menjadi lebih simpel dan spesifik. Eliminasi perhitungan-perhitungan yang tidak diperlukan, karena ketika menggunakan metode pohon keputusan maka sample diuji hanya berdasarkan kriteria atau kelas tertentu. Fleksibel untuk memilih fitur dari internal node yang berbeda, fitur yang terpilih akan membedakan suatu kriteria dibandingkan kriteria yang lain dalam node yang sama. Kefleksibelan metode pohon keputusan ini meningkatkan kualitas keputusan yang dihasilkan jika dibandingkan ketika menggunakan metode penghitungan satu tahap yang lebih konvensional. Dalam analisis multivariat, dengan kriteria dan kelas yang jumlahnya sangat banyak, seorang penguji biasanya perlu untuk mengestimasikan baik itu distribusi dimensi tinggi ataupun parameter tertentu dari distribusi kelas tersebut. Metode pohon keputusan dapat menghindari munculnya permasalahan ini dengan menggunakan criteria yang jumlahnya lebih sedikit pada setiap node internal tanpa banyak mengurangi kualitas keputusan yang dihasilkan. Kekurangan Pohon Keputusan \u00b6 Terjadi overlap terutama ketika kelas-kelas dan criteria yang digunakan jumlahnya sangat banyak. Hal tersebut juga dapat menyebabkan meningkatnya waktu pengambilan keputusan dan jumlah memori yang diperlukan. Pengakumulasian jumlah eror dari setiap tingkat dalam sebuah pohon keputusan yang besar. Kesulitan dalam mendesain pohon keputusan yang optimal. Hasil kualitas keputusan yang didapatkan dari metode pohon keputusan sangat tergantung pada bagaimana pohon tersebut didesain. Model Pohon Keputusan Disini setiap percabangan menyatakan kondisi yang harus dipenuhi dan tiap ujung pohon menyatakan kelas data. Contoh di Gambar diatas adalah identifikasi pembeli komputer,dari pohon keputusan tersebut diketahui bahwa salah satu kelompok yang potensial membeli komputer adalah orang yang berusia di bawah 30 tahun dan juga pelajar. Setelah sebuah pohon keputusan dibangun maka dapat digunakan untuk mengklasifikasikan record yang belum ada kelasnya. Dimulai dari node root , menggunakan tes terhadap atribut dari record yang belum ada kelasnya tersebut lalu mengikuti cabang yang sesuai dengan hasil dari tes tersebut, yang akan membawa kepada internal node ( node yang memiliki satu cabang masuk dan dua atau lebih cabang yang keluar), dengan cara harus melakukan tes lagi terhadap atribut atau node daun. Record yang kelasnya tidak diketahui kemudian diberikan kelas yang sesuai dengan kelas yang ada pada node daun. Pada pohon keputusan setiap simpul daun menandai label kelas. Proses dalam pohon keputusan yaitu mengubah bentuk data (tabel) menjadi model pohon ( tree ) kemudian mengubah model pohon tersebut menjadi aturan ( rule ). ALGORITMA C4.5 Pohon dibangun dengan cara membagi data secara rekursif hingga tiap bagian terdiri dari data yang berasal dari kelas yang sama. Bentuk pemecahan ( split) yang digunakan untuk membagi data tergantung dari jenis atribut yang digunakan dalam split . Algoritma C4.5 dapat menangani data numerik (kontinyu) dan diskret. Split untuk atribut numerik yaitu mengurutkan contoh berdasarkan atribut kontiyu A, kemudian membentuk minimum permulaan (threshold ) M dari contoh-contoh yang ada dari kelas mayoritas pada setiap partisi yang bersebelahan, lalu menggabungkan partisi-partisi yang bersebelahan tersebut dengan kelas mayoritas yang sama. Split untuk atribut diskret A mempunyai bentuk value (A) \u03b5 X dimana X \u2282 domain(A) . Jika suatu set data mempunyai beberapa pengamatan dengan missing value yaitu record dengan beberapa nilai variabel tidak ada, Jika jumlah pengamatan terbatas maka atribut dengan missing value dapat diganti dengan nilai rata-rata dari variabel yang bersangkutan.[Santosa,2007] Untuk melakukan pemisahan obyek ( split) dilakukan tes terhadap atribut dengan mengukur tingkat ketidakmurnian pada sebuah simpul ( node) . Pada algoritma C.45 menggunakan rasio perolehan ( gain ratio ). Sebelum menghitung rasio perolehan, perlu menghitung dulu nilai informasi dalam satuan bits dari suatu kumpulan objek. Cara menghitungnya dilakukan dengan menggunakan konsep entropi. S adalah ruang (data) sampel yang digunakan untuk pelatihan, p+ adalah jumlah yang bersolusi positif atau mendukung pada data sampel untuk kriteria tertentu dan p- adalah jumlah yang bersolusi negatif atau tidak mendukung pada data sampel untuk kriteria tertentu. ntropi( S ) sama dengan 0, jika semua contoh pada S berada dalam kelas yang sama. Entropi(S) sama dengan 1, jika jumlah contoh positif dan negative dalam S adalah sama. Entropi(S) lebih dari 0 tetapi kurang dari 1, jika jumlah contoh positif dan negative dalam S tidak sama [Mitchell,1997].Entropi split yang membagi S dengan n record menjadi himpunan-himpunan S1 dengan n1 baris dan S2 dengan n2 baris adalah : Kemudian menghitung perolehan informasi dari output data atau variabel dependent y yang dikelompokkan berdasarkan atribut A, dinotasikan dengan gain ( y ,A). Perolehan informasi*, gain* ( y ,A), dari atribut A relative terhadap output data y adalah: nilai (A) adalah semua nilai yang mungkin dari atribut A, dan y*c adalah subset dari y dimana A mempunyai nilai c. Term pertama dalam persamaan diatas adalah *entropy total y dan term kedua adalah entropy sesudah dilakukan pemisahan data berdasarkan atribut A. Untuk menghitung rasio perolehan perlu diketahui suatu term baru yang disebut pemisahan informasi (SplitInfo ). Pemisahan informasi dihitung dengan cara : bahwa S1 sampai Sc adalah c subset yang dihasilkan dari pemecahan S dengan menggunakan atribut A yang mempunyai sebanyak c nilai. Selanjutnya rasio perolehan (gain ratio) dihitung dengan cara : Study Kasus (decision tree univeres) \u00b6 # In[1]: import pandas as pd from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import train_test_split from sklearn.tree import export_graphviz from sklearn.externals.six import StringIO from IPython.display import Image from sklearn import metrics import pydotplus import numpy as np # In[4]: data = pd . read_csv ( 'UniversalBank.csv' ) data . head () # In[5]: data . info () # In[6]: zero_not_accepted = [ 'ID' , 'Age' , 'Experience' , 'Income' , 'ZIP Code' , 'Family' , 'CCAvg' , 'Education' , 'Mortgage' , 'Personal Loan' , 'Personal Loan' , 'Securities Account' , 'CD Account' ] # for col in zero_not_accepted: # for i in data[col]: # if i==0: # colSum = sum(data[col]) # meanCol=colSum/len(data[col]) # data[col]=meanCol for col in zero_not_accepted : data [ col ] = data [ col ] . replace ( 0 , np . NaN ) mean = int ( data [ col ] . mean ( skipna = True )) data [ col ] = data [ col ] . replace ( np . NaN , mean ) # In[8]: X = data [[ 'ID' , 'Age' , 'Experience' , 'Income' , 'ZIP Code' , 'Family' , 'CCAvg' , 'Education' , 'Mortgage' , 'Personal Loan' , 'Personal Loan' , 'Securities Account' , 'CD Account' ]] y = data [ 'Online' ] #memecah data X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.3 , random_state = 0 ) # In[13]: # Buat objek classifer Pohon Keputusan clf = DecisionTreeClassifier ( criterion = \"entropy\" , max_depth = 3 ) # Melatih Pohon Pengambilan Keputusan clf = clf . fit ( X_train , y_train ) #Prediksikan respons untuk dataset uji y_pred = clf . predict ( X_test ) # Model Accuracy, seberapa sering penggolongnya benar? print ( \"Accuracy:\" , metrics . accuracy_score ( y_test , y_pred )) # In[14]: feature_cols = [ 'ID' , 'Age' , 'Experience' , 'Income' , 'ZIP Code' , 'Family' , 'CCAvg' , 'Education' , 'Mortgage' , 'Personal Loan' , 'Personal Loan' , 'Securities Account' , 'CD Account' ] dot_data = StringIO () export_graphviz ( clf , out_file = dot_data , filled = True , rounded = True , special_characters = True , feature_names = feature_cols , class_names = [ '0' , '1' ]) graph = pydotplus . graph_from_dot_data ( dot_data . getvalue ()) graph . write_png ( 'bank.png' ) Image ( graph . create_png ()) # In[ ]: Referensi \u00b6 https://fairuzelsaid.wordpress.com/2009/11/24/data-mining-konsep-pohon-keputusan/ http://nugikkool.blogspot.com/2012/08/pohon-keputusan-id3-dan-c45-menggunakan.html","title":"Decision Tree"},{"location":"DECISION TREE  POHON KEPUTUSAN/#decision-tree-pohon-keputusan","text":"","title":"DECISION TREE / POHON KEPUTUSAN"},{"location":"DECISION TREE  POHON KEPUTUSAN/#pengertian-pohon-keputusan","text":"Salah satu metode data mining yang umum digunakan adalah pohon keputusan . Metode pohon keputusan mengubah fakta yang sangat besar menjadi pohon keputusan yang merepresentasikan rule. Pohon keputusan adalah salah satu metode klasifikasi yang paling popular karena mudah untuk diinterpretasi oleh manusia. Konsep dari pohon keputusan adalah mengubah data menjadi pohon keputusan dan aturan-aturan keputusan. Konsep Pohon Keputusan Data dalam pohon keputusan biasanya dinyatakan dalam bentuk tabel dengan atribut dan record. Atribut menyatakan suatu parameter yang dibuat sebagai kriteria dalam pembentukan tree. Konsep Data dalam Pohon Keputusan Proses pada pohon keputusan adalah mengubah bentuk data (tabel) menjadi model pohon, mengubah model pohon menjadi rule, dan menyederhanakan rule.","title":"Pengertian Pohon Keputusan"},{"location":"DECISION TREE  POHON KEPUTUSAN/#manfaat-pohon-keputusan","text":"Manfaat utama dari penggunaan pohon keputusan adalah kemampuannya untuk mem- break down proses pengambilan keputusan yang kompleks menjadi lebih simpel sehingga pengambil keputusan akan lebih menginterpretasikan solusi dari permasalahan. Pohon Keputusan juga berguna untuk mengeksplorasi data, menemukan hubungan tersembunyi antara sejumlah calon variabel input dengan sebuah variabel target. Pohon keputusan memadukan antara eksplorasi data dan pemodelan, sehingga sangat bagus sebagai langkah awal dalam proses pemodelan bahkan ketika dijadikan sebagai model akhir dari beberapa teknik lain. Sering terjadi tawar menawar antara keakuratan model dengan transparansi model. Dalam beberapa aplikasi, akurasi dari sebuah klasifikasi atau prediksi adalah satu-satunya hal yang ditonjolkan, misalnya sebuah perusahaan direct mail membuat sebuah model yang akurat untuk memprediksi anggota mana yang berpotensi untuk merespon permintaan, tanpa memperhatikan bagaimana atau mengapa model tersebut bekerja. Konsep Dasar Pohon Keputusan Bagian awal dari pohon keputusan ini adalah titik akar (root), sedangkan setiap cabang dari pohon keputusan merupakan pembagian berdasarkan hasil uji, dan titik akhir (leaf) merupakan pembagian kelas yang dihasilkan. Pohon keputusan mempunyai 3 tipe simpul yaitu: Simpul akar, dimana tidak memiliki cabang yang masuk dan memiliki cabang lebih dari satu, terkadang tidak memiliki cabang sama sekali. Simpul ini biasanya berupa atribut yang paling memiliki pengaruh terbesar pada suatu kelas tertentu. Simpul internal, dimana hanya memiliki 1 cabang yang masuk, dan memiliki lebih dari 1 cabang yang keluar. Simpul daun, atau simpul akhir dimana hanya memiliki 1 cabang yang masuk, dan tidak memiliki cabang sama sekali dan menandai bahwa simpul tersebut merupakan label kelas. Tahap awal dilakukan pengujian simpul akar, jika pada pengujian simpul akar menghasilkan sesuatu maka proses pengujian juga dilakukan pada setiap cabang berdasarkan hasil dari pengujian. Hal ini berlaku juga untuk simpul internal dimana suatu kondisi pengujian baru akan diterapkan pada simpul daun. Pada umumnya proses dari sistem pohon keputusan adalah mengadopsi strategi pencarian top-down untuk solusi ruang pencariannya. Pada proses mengklasifikasikan sampel yang tidak diketahui, nilai atribut akan diuji pada pohon keputusan dengan cara melacak jalur dari titik akar sampai titik akhir, kemudian akan diprediksikan kelas yang ditempati sampel baru tersebut.","title":"Manfaat Pohon Keputusan"},{"location":"DECISION TREE  POHON KEPUTUSAN/#kelebihan-pohon-keputusan","text":"Kelebihan dari metode pohon keputusan adalah: Daerah pengambilan keputusan yang sebelumnya kompleks dan sangat global, dapat diubah menjadi lebih simpel dan spesifik. Eliminasi perhitungan-perhitungan yang tidak diperlukan, karena ketika menggunakan metode pohon keputusan maka sample diuji hanya berdasarkan kriteria atau kelas tertentu. Fleksibel untuk memilih fitur dari internal node yang berbeda, fitur yang terpilih akan membedakan suatu kriteria dibandingkan kriteria yang lain dalam node yang sama. Kefleksibelan metode pohon keputusan ini meningkatkan kualitas keputusan yang dihasilkan jika dibandingkan ketika menggunakan metode penghitungan satu tahap yang lebih konvensional. Dalam analisis multivariat, dengan kriteria dan kelas yang jumlahnya sangat banyak, seorang penguji biasanya perlu untuk mengestimasikan baik itu distribusi dimensi tinggi ataupun parameter tertentu dari distribusi kelas tersebut. Metode pohon keputusan dapat menghindari munculnya permasalahan ini dengan menggunakan criteria yang jumlahnya lebih sedikit pada setiap node internal tanpa banyak mengurangi kualitas keputusan yang dihasilkan.","title":"Kelebihan Pohon Keputusan"},{"location":"DECISION TREE  POHON KEPUTUSAN/#kekurangan-pohon-keputusan","text":"Terjadi overlap terutama ketika kelas-kelas dan criteria yang digunakan jumlahnya sangat banyak. Hal tersebut juga dapat menyebabkan meningkatnya waktu pengambilan keputusan dan jumlah memori yang diperlukan. Pengakumulasian jumlah eror dari setiap tingkat dalam sebuah pohon keputusan yang besar. Kesulitan dalam mendesain pohon keputusan yang optimal. Hasil kualitas keputusan yang didapatkan dari metode pohon keputusan sangat tergantung pada bagaimana pohon tersebut didesain. Model Pohon Keputusan Disini setiap percabangan menyatakan kondisi yang harus dipenuhi dan tiap ujung pohon menyatakan kelas data. Contoh di Gambar diatas adalah identifikasi pembeli komputer,dari pohon keputusan tersebut diketahui bahwa salah satu kelompok yang potensial membeli komputer adalah orang yang berusia di bawah 30 tahun dan juga pelajar. Setelah sebuah pohon keputusan dibangun maka dapat digunakan untuk mengklasifikasikan record yang belum ada kelasnya. Dimulai dari node root , menggunakan tes terhadap atribut dari record yang belum ada kelasnya tersebut lalu mengikuti cabang yang sesuai dengan hasil dari tes tersebut, yang akan membawa kepada internal node ( node yang memiliki satu cabang masuk dan dua atau lebih cabang yang keluar), dengan cara harus melakukan tes lagi terhadap atribut atau node daun. Record yang kelasnya tidak diketahui kemudian diberikan kelas yang sesuai dengan kelas yang ada pada node daun. Pada pohon keputusan setiap simpul daun menandai label kelas. Proses dalam pohon keputusan yaitu mengubah bentuk data (tabel) menjadi model pohon ( tree ) kemudian mengubah model pohon tersebut menjadi aturan ( rule ). ALGORITMA C4.5 Pohon dibangun dengan cara membagi data secara rekursif hingga tiap bagian terdiri dari data yang berasal dari kelas yang sama. Bentuk pemecahan ( split) yang digunakan untuk membagi data tergantung dari jenis atribut yang digunakan dalam split . Algoritma C4.5 dapat menangani data numerik (kontinyu) dan diskret. Split untuk atribut numerik yaitu mengurutkan contoh berdasarkan atribut kontiyu A, kemudian membentuk minimum permulaan (threshold ) M dari contoh-contoh yang ada dari kelas mayoritas pada setiap partisi yang bersebelahan, lalu menggabungkan partisi-partisi yang bersebelahan tersebut dengan kelas mayoritas yang sama. Split untuk atribut diskret A mempunyai bentuk value (A) \u03b5 X dimana X \u2282 domain(A) . Jika suatu set data mempunyai beberapa pengamatan dengan missing value yaitu record dengan beberapa nilai variabel tidak ada, Jika jumlah pengamatan terbatas maka atribut dengan missing value dapat diganti dengan nilai rata-rata dari variabel yang bersangkutan.[Santosa,2007] Untuk melakukan pemisahan obyek ( split) dilakukan tes terhadap atribut dengan mengukur tingkat ketidakmurnian pada sebuah simpul ( node) . Pada algoritma C.45 menggunakan rasio perolehan ( gain ratio ). Sebelum menghitung rasio perolehan, perlu menghitung dulu nilai informasi dalam satuan bits dari suatu kumpulan objek. Cara menghitungnya dilakukan dengan menggunakan konsep entropi. S adalah ruang (data) sampel yang digunakan untuk pelatihan, p+ adalah jumlah yang bersolusi positif atau mendukung pada data sampel untuk kriteria tertentu dan p- adalah jumlah yang bersolusi negatif atau tidak mendukung pada data sampel untuk kriteria tertentu. ntropi( S ) sama dengan 0, jika semua contoh pada S berada dalam kelas yang sama. Entropi(S) sama dengan 1, jika jumlah contoh positif dan negative dalam S adalah sama. Entropi(S) lebih dari 0 tetapi kurang dari 1, jika jumlah contoh positif dan negative dalam S tidak sama [Mitchell,1997].Entropi split yang membagi S dengan n record menjadi himpunan-himpunan S1 dengan n1 baris dan S2 dengan n2 baris adalah : Kemudian menghitung perolehan informasi dari output data atau variabel dependent y yang dikelompokkan berdasarkan atribut A, dinotasikan dengan gain ( y ,A). Perolehan informasi*, gain* ( y ,A), dari atribut A relative terhadap output data y adalah: nilai (A) adalah semua nilai yang mungkin dari atribut A, dan y*c adalah subset dari y dimana A mempunyai nilai c. Term pertama dalam persamaan diatas adalah *entropy total y dan term kedua adalah entropy sesudah dilakukan pemisahan data berdasarkan atribut A. Untuk menghitung rasio perolehan perlu diketahui suatu term baru yang disebut pemisahan informasi (SplitInfo ). Pemisahan informasi dihitung dengan cara : bahwa S1 sampai Sc adalah c subset yang dihasilkan dari pemecahan S dengan menggunakan atribut A yang mempunyai sebanyak c nilai. Selanjutnya rasio perolehan (gain ratio) dihitung dengan cara :","title":"Kekurangan Pohon Keputusan"},{"location":"DECISION TREE  POHON KEPUTUSAN/#study-kasus-decision-tree-univeres","text":"# In[1]: import pandas as pd from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import train_test_split from sklearn.tree import export_graphviz from sklearn.externals.six import StringIO from IPython.display import Image from sklearn import metrics import pydotplus import numpy as np # In[4]: data = pd . read_csv ( 'UniversalBank.csv' ) data . head () # In[5]: data . info () # In[6]: zero_not_accepted = [ 'ID' , 'Age' , 'Experience' , 'Income' , 'ZIP Code' , 'Family' , 'CCAvg' , 'Education' , 'Mortgage' , 'Personal Loan' , 'Personal Loan' , 'Securities Account' , 'CD Account' ] # for col in zero_not_accepted: # for i in data[col]: # if i==0: # colSum = sum(data[col]) # meanCol=colSum/len(data[col]) # data[col]=meanCol for col in zero_not_accepted : data [ col ] = data [ col ] . replace ( 0 , np . NaN ) mean = int ( data [ col ] . mean ( skipna = True )) data [ col ] = data [ col ] . replace ( np . NaN , mean ) # In[8]: X = data [[ 'ID' , 'Age' , 'Experience' , 'Income' , 'ZIP Code' , 'Family' , 'CCAvg' , 'Education' , 'Mortgage' , 'Personal Loan' , 'Personal Loan' , 'Securities Account' , 'CD Account' ]] y = data [ 'Online' ] #memecah data X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.3 , random_state = 0 ) # In[13]: # Buat objek classifer Pohon Keputusan clf = DecisionTreeClassifier ( criterion = \"entropy\" , max_depth = 3 ) # Melatih Pohon Pengambilan Keputusan clf = clf . fit ( X_train , y_train ) #Prediksikan respons untuk dataset uji y_pred = clf . predict ( X_test ) # Model Accuracy, seberapa sering penggolongnya benar? print ( \"Accuracy:\" , metrics . accuracy_score ( y_test , y_pred )) # In[14]: feature_cols = [ 'ID' , 'Age' , 'Experience' , 'Income' , 'ZIP Code' , 'Family' , 'CCAvg' , 'Education' , 'Mortgage' , 'Personal Loan' , 'Personal Loan' , 'Securities Account' , 'CD Account' ] dot_data = StringIO () export_graphviz ( clf , out_file = dot_data , filled = True , rounded = True , special_characters = True , feature_names = feature_cols , class_names = [ '0' , '1' ]) graph = pydotplus . graph_from_dot_data ( dot_data . getvalue ()) graph . write_png ( 'bank.png' ) Image ( graph . create_png ()) # In[ ]:","title":"Study Kasus (decision tree univeres)"},{"location":"DECISION TREE  POHON KEPUTUSAN/#referensi","text":"https://fairuzelsaid.wordpress.com/2009/11/24/data-mining-konsep-pohon-keputusan/ http://nugikkool.blogspot.com/2012/08/pohon-keputusan-id3-dan-c45-menggunakan.html","title":"Referensi"},{"location":"K-Nearest Neighbor (KNN)/","text":"K-Nearest Neighbor (KNN) \u00b6 Pengertian K-NN \u00b6 Algoritma K-NN adalah sebuah metode untuk melakukan klasifikasi terhadap objek berdasarkan data pembelajaran yang jaraknya paling dekat dengan objek tersebut. Ketepatan algoritme K-NN ini sangat dipengaruhi oleh ada atau tidaknya fitur-fitur yang tidak relevan, atau jika bobot fitur tersebut tidak setara dengan relevansinya terhadap klasifikasi. Riset terhadap algoritme ini sebagian besar membahas bagaimana memilih dan memberi bobot terhadap fitur, agar performa klasifikasi menjadi lebih baik. Terdapat beberapa jenis algoritme pencarian tetangga terdekat, diantaranya: Linear scan Pohon kd Pohon Balltree Pohon metric Locally-sensitive hashing (LSH) Algoritme K-NN ini memiliki konsistensi yang kuat. Ketika jumlah data mendekati tak hingga, algoritme ini menjamin error rate yang tidak lebih dari dua kali Bayes error rate ( error rate minimum untuk distribusi data tertentu). Data untuk algoritma KNN terdiri dari beberapa atribut multi-variate Xi yang akan digunakan untuk mengklasifikasikan Y. Data dari KNN dapat dalam skala ukuran apapun, dari ordinal ke nominal. Untuk rumus yang digunakan, untuk menghitung jaraknya kita bisa gunakan rumus Euclidean Distance. Mirip dengan Pythagoras, hanya saja Euclidean Distance memiliki dimensi lebih dari 2. Tahapan Langkah Algoritma K-NN \u00b6 Menentukan parameter k (jumlah tetangga paling dekat). Menghitung kuadrat jarak eucliden objek terhadap data training yang diberikan. Mengurutkan hasil no 2 secara ascending (berurutan dari nilai tinggi ke rendah). Mengumpulkan kategori Y (klasifikasi nearest neighbor berdasarkan nilai k). Dengan menggunakan kategori nearest neighbor yang paling mayoritas maka dapat diprediksikan kategori objek. Kelebihan dan Kekurangan K-NN \u00b6 Kelebihan KNN memiliki beberapa kelebihan yaitu bahwa dia tangguh terhadap training data yang noisy dan efektif apabila data latih nya besar. Kekurangan KNN perlu menentukan nilai dari parameter K (jumlah dari tetangga terdekat). Pembelajaran berdasarkan jarak tidak jelas mengenai jenis jarak apa yang harus digunakan dan atribut mana yang harus digunakan untuk mendapatkan hasil yang terbaik. Biaya komputasi cukup tinggi karena diperlukan perhitungan jarak dari tiap sample uji pada keseluruhan sample latih. Implementasi \u00b6 #import library import math import pandas as pd import matplotlib.pyplot as plt #memasukkan file csv dari excel ke python dataset = pd . read_csv ( 'UniversalBank.csv' ) col_age = dataset . iloc [:, 1 ] . values #mengambil kolom 2 dari excel (Age) col_exp = dataset . iloc [:, 2 ] . values #mengambil kolom 3 dari excel (Experience) col_inc = dataset . iloc [:, 3 ] . values #mengambil kolom 4 dari excel (Income) col_zip = dataset . iloc [:, 4 ] . values #mengambil kolom 5 dari excel (Zip Code) col_fam = dataset . iloc [:, 5 ] . values #mengambil kolom 6 dari excel (Family) col_cca = dataset . iloc [:, 6 ] . values #mengambil kolom 7 dari excel (CCAvg) col_edu = dataset . iloc [:, 7 ] . values #mengambil kolom 8 dari excel (Education) col_onl = dataset . iloc [:, 12 ] . values #mengambil kolom 13 dari excel (Online) knn = int ( input ( \"Masukkan Nilai K = \" )) age = [] exp = [] inc = [] zip1 = [] fam = [] cca = [] edu = [] online = [] age_dt = [] exp_dt = [] inc_dt = [] zip1_dt = [] fam_dt = [] cca_dt = [] edu_dt = [] onl_dt = [] Hasil = [] benar = [] data = [] accuracy = [] #fungsi mengambil data train yang mempunyai nilai class 0 dengan nama masuk_data_train_0 def masuk_data_train_0 ( data , masuk ): a = 0 for i in range ( len ( data )): if ( col_onl [ i ] == 0 and a < 202 ): masuk . append ( data [ i ]) a = a + 1 #fungsi mengambil data train yang mempunyai nilai class 1 dengan nama masuk_data_train_1 def masuk_data_train_1 ( data , masuk ): a = 0 for i in range ( len ( data )): if ( col_onl [ i ] == 1 and a < 298 ): masuk . append ( data [ i ]) a = a + 1 #fungsi mengambil data tes yang mempunyai nilai class 0 dengan nama masuk_data_test_0 def masuk_data_test_0 ( data , masuk ): a = 0 for i in range ( len ( data )): if ( col_onl [ i ] == 0 ): a += 1 if ( a > 202 ): masuk . append ( data [ i ]) #fungsi mengambil data tes yang mempunyai nilai class 1 dengan nama masuk_data_test_1 def masuk_data_test_1 ( data , masuk ): a = 0 for i in range ( len ( data )): if ( col_onl [ i ] == 1 ): a += 1 if ( a > 298 ): masuk . append ( data [ i ]) #fungsi mengambil data dari beberapa variable data train, data tes, knn, dan menampilkan hasil def pcx ( data2 , data3 , data4 , data5 , data6 , data7 , data8 , data14 , dt2 , dt3 , dt4 , dt5 , dt6 , dt7 , dt8 , dt14 , k , out ): #membuat perulangan untuk menghitung jarak dari data asli dengan data tes for i in range ( len ( dt2 )): dist1 = [] online = [] coba = 0 for a in range ( len ( data2 )): dist = math . sqrt ( (( data2 [ a ] - dt2 [ i ]) ** 2 ) + (( data3 [ a ] - dt3 [ i ]) ** 2 ) + (( data4 [ a ] - dt4 [ i ]) ** 2 ) + (( data5 [ a ] - dt5 [ i ]) ** 2 ) + (( data6 [ a ] - dt6 [ i ]) ** 2 ) + (( data7 [ a ] - dt7 [ i ]) ** 2 ) + (( data8 [ a ] - dt8 [ i ]) ** 2 )) dist1 . append ( dist ) dist1 , online = zip ( * sorted ( zip ( dist1 , data14 ))) for z in range ( k ): if ( online [ z ] == 0 ) : coba += 1 if (( z / 2 ) <= coba ): a = 0 out . append ( a ) else : a = 1 out . append ( a ) del dist1 del online coba = 0 def hasil ( data_asli , data_perbandingan , out , out2 , out3 ): a = 0 for x in range ( len ( data_asli )): if ( data_asli [ x ] == data_perbandingan [ x ]): a += 1 out . append ( a ) out2 . append ( 4500 ) out3 . append ( a / 4500 ) #memasukkan data train mempunyai nilai class 0 masuk_data_train_0 ( col_age , age ) masuk_data_train_0 ( col_exp , exp ) masuk_data_train_0 ( col_inc , inc ) masuk_data_train_0 ( col_zip , zip1 ) masuk_data_train_0 ( col_fam , fam ) masuk_data_train_0 ( col_cca , cca ) masuk_data_train_0 ( col_edu , edu ) masuk_data_train_0 ( col_onl , online ) #memasukkan data train mempunyai nilai class 1 masuk_data_train_1 ( col_age , age ) masuk_data_train_1 ( col_exp , exp ) masuk_data_train_1 ( col_inc , inc ) masuk_data_train_1 ( col_zip , zip1 ) masuk_data_train_1 ( col_fam , fam ) masuk_data_train_1 ( col_cca , cca ) masuk_data_train_1 ( col_edu , edu ) masuk_data_train_1 ( col_onl , online ) #memasukkan data tes mempunyai nilai class 0 masuk_data_test_0 ( col_age , age_dt ) masuk_data_test_0 ( col_exp , exp_dt ) masuk_data_test_0 ( col_inc , inc_dt ) masuk_data_test_0 ( col_zip , zip1_dt ) masuk_data_test_0 ( col_fam , fam_dt ) masuk_data_test_0 ( col_cca , cca_dt ) masuk_data_test_0 ( col_edu , edu_dt ) masuk_data_test_0 ( col_onl , onl_dt ) #memasukkan data train mempunyai nilai class 1 masuk_data_test_1 ( col_age , age_dt ) masuk_data_test_1 ( col_exp , exp_dt ) masuk_data_test_1 ( col_inc , inc_dt ) masuk_data_test_1 ( col_zip , zip1_dt ) masuk_data_test_1 ( col_fam , fam_dt ) masuk_data_test_1 ( col_cca , cca_dt ) masuk_data_test_1 ( col_edu , edu_dt ) masuk_data_test_1 ( col_onl , onl_dt ) pcx ( age , exp , inc , zip1 , fam , cca , edu , online , age_dt , exp_dt , inc_dt , zip1_dt , fam_dt , cca_dt , edu_dt , onl_dt , knn , Hasil ) hasil ( onl_dt , Hasil , benar , data , accuracy ) df = pd . DataFrame ({ 'knn' : knn , 'databenar' : benar , 'Jmldata' : data , 'accuracy' : accuracy }) df . plot ( kind = 'bar' , x = 'knn' , y = 'accuracy' , color = 'blue' ) df . sort_values ( by = [ 'databenar' ], inplace = True , ascending = False ) print ( df ) plt . show () Referensi \u00b6 https://id.wikipedia.org/wiki/KNN https://www.advernesia.com/blog/data-science/pengertian-dan-cara-kerja-algoritma-k-nearest-neighbours-knn/ https://medium.com/bee-solution-partners/cara-kerja-algoritma-k-nearest-neighbor-k-nn-389297de543e http://cgeduntuksemua.blogspot.com/2012/03/pengertian-kelebihan-dan-kekurangan-k.html","title":"K-Nearest Neighbor"},{"location":"K-Nearest Neighbor (KNN)/#k-nearest-neighbor-knn","text":"","title":"K-Nearest Neighbor (KNN)"},{"location":"K-Nearest Neighbor (KNN)/#pengertian-k-nn","text":"Algoritma K-NN adalah sebuah metode untuk melakukan klasifikasi terhadap objek berdasarkan data pembelajaran yang jaraknya paling dekat dengan objek tersebut. Ketepatan algoritme K-NN ini sangat dipengaruhi oleh ada atau tidaknya fitur-fitur yang tidak relevan, atau jika bobot fitur tersebut tidak setara dengan relevansinya terhadap klasifikasi. Riset terhadap algoritme ini sebagian besar membahas bagaimana memilih dan memberi bobot terhadap fitur, agar performa klasifikasi menjadi lebih baik. Terdapat beberapa jenis algoritme pencarian tetangga terdekat, diantaranya: Linear scan Pohon kd Pohon Balltree Pohon metric Locally-sensitive hashing (LSH) Algoritme K-NN ini memiliki konsistensi yang kuat. Ketika jumlah data mendekati tak hingga, algoritme ini menjamin error rate yang tidak lebih dari dua kali Bayes error rate ( error rate minimum untuk distribusi data tertentu). Data untuk algoritma KNN terdiri dari beberapa atribut multi-variate Xi yang akan digunakan untuk mengklasifikasikan Y. Data dari KNN dapat dalam skala ukuran apapun, dari ordinal ke nominal. Untuk rumus yang digunakan, untuk menghitung jaraknya kita bisa gunakan rumus Euclidean Distance. Mirip dengan Pythagoras, hanya saja Euclidean Distance memiliki dimensi lebih dari 2.","title":"Pengertian K-NN"},{"location":"K-Nearest Neighbor (KNN)/#tahapan-langkah-algoritma-k-nn","text":"Menentukan parameter k (jumlah tetangga paling dekat). Menghitung kuadrat jarak eucliden objek terhadap data training yang diberikan. Mengurutkan hasil no 2 secara ascending (berurutan dari nilai tinggi ke rendah). Mengumpulkan kategori Y (klasifikasi nearest neighbor berdasarkan nilai k). Dengan menggunakan kategori nearest neighbor yang paling mayoritas maka dapat diprediksikan kategori objek.","title":"Tahapan Langkah Algoritma K-NN"},{"location":"K-Nearest Neighbor (KNN)/#kelebihan-dan-kekurangan-k-nn","text":"Kelebihan KNN memiliki beberapa kelebihan yaitu bahwa dia tangguh terhadap training data yang noisy dan efektif apabila data latih nya besar. Kekurangan KNN perlu menentukan nilai dari parameter K (jumlah dari tetangga terdekat). Pembelajaran berdasarkan jarak tidak jelas mengenai jenis jarak apa yang harus digunakan dan atribut mana yang harus digunakan untuk mendapatkan hasil yang terbaik. Biaya komputasi cukup tinggi karena diperlukan perhitungan jarak dari tiap sample uji pada keseluruhan sample latih.","title":"Kelebihan dan Kekurangan K-NN"},{"location":"K-Nearest Neighbor (KNN)/#implementasi","text":"#import library import math import pandas as pd import matplotlib.pyplot as plt #memasukkan file csv dari excel ke python dataset = pd . read_csv ( 'UniversalBank.csv' ) col_age = dataset . iloc [:, 1 ] . values #mengambil kolom 2 dari excel (Age) col_exp = dataset . iloc [:, 2 ] . values #mengambil kolom 3 dari excel (Experience) col_inc = dataset . iloc [:, 3 ] . values #mengambil kolom 4 dari excel (Income) col_zip = dataset . iloc [:, 4 ] . values #mengambil kolom 5 dari excel (Zip Code) col_fam = dataset . iloc [:, 5 ] . values #mengambil kolom 6 dari excel (Family) col_cca = dataset . iloc [:, 6 ] . values #mengambil kolom 7 dari excel (CCAvg) col_edu = dataset . iloc [:, 7 ] . values #mengambil kolom 8 dari excel (Education) col_onl = dataset . iloc [:, 12 ] . values #mengambil kolom 13 dari excel (Online) knn = int ( input ( \"Masukkan Nilai K = \" )) age = [] exp = [] inc = [] zip1 = [] fam = [] cca = [] edu = [] online = [] age_dt = [] exp_dt = [] inc_dt = [] zip1_dt = [] fam_dt = [] cca_dt = [] edu_dt = [] onl_dt = [] Hasil = [] benar = [] data = [] accuracy = [] #fungsi mengambil data train yang mempunyai nilai class 0 dengan nama masuk_data_train_0 def masuk_data_train_0 ( data , masuk ): a = 0 for i in range ( len ( data )): if ( col_onl [ i ] == 0 and a < 202 ): masuk . append ( data [ i ]) a = a + 1 #fungsi mengambil data train yang mempunyai nilai class 1 dengan nama masuk_data_train_1 def masuk_data_train_1 ( data , masuk ): a = 0 for i in range ( len ( data )): if ( col_onl [ i ] == 1 and a < 298 ): masuk . append ( data [ i ]) a = a + 1 #fungsi mengambil data tes yang mempunyai nilai class 0 dengan nama masuk_data_test_0 def masuk_data_test_0 ( data , masuk ): a = 0 for i in range ( len ( data )): if ( col_onl [ i ] == 0 ): a += 1 if ( a > 202 ): masuk . append ( data [ i ]) #fungsi mengambil data tes yang mempunyai nilai class 1 dengan nama masuk_data_test_1 def masuk_data_test_1 ( data , masuk ): a = 0 for i in range ( len ( data )): if ( col_onl [ i ] == 1 ): a += 1 if ( a > 298 ): masuk . append ( data [ i ]) #fungsi mengambil data dari beberapa variable data train, data tes, knn, dan menampilkan hasil def pcx ( data2 , data3 , data4 , data5 , data6 , data7 , data8 , data14 , dt2 , dt3 , dt4 , dt5 , dt6 , dt7 , dt8 , dt14 , k , out ): #membuat perulangan untuk menghitung jarak dari data asli dengan data tes for i in range ( len ( dt2 )): dist1 = [] online = [] coba = 0 for a in range ( len ( data2 )): dist = math . sqrt ( (( data2 [ a ] - dt2 [ i ]) ** 2 ) + (( data3 [ a ] - dt3 [ i ]) ** 2 ) + (( data4 [ a ] - dt4 [ i ]) ** 2 ) + (( data5 [ a ] - dt5 [ i ]) ** 2 ) + (( data6 [ a ] - dt6 [ i ]) ** 2 ) + (( data7 [ a ] - dt7 [ i ]) ** 2 ) + (( data8 [ a ] - dt8 [ i ]) ** 2 )) dist1 . append ( dist ) dist1 , online = zip ( * sorted ( zip ( dist1 , data14 ))) for z in range ( k ): if ( online [ z ] == 0 ) : coba += 1 if (( z / 2 ) <= coba ): a = 0 out . append ( a ) else : a = 1 out . append ( a ) del dist1 del online coba = 0 def hasil ( data_asli , data_perbandingan , out , out2 , out3 ): a = 0 for x in range ( len ( data_asli )): if ( data_asli [ x ] == data_perbandingan [ x ]): a += 1 out . append ( a ) out2 . append ( 4500 ) out3 . append ( a / 4500 ) #memasukkan data train mempunyai nilai class 0 masuk_data_train_0 ( col_age , age ) masuk_data_train_0 ( col_exp , exp ) masuk_data_train_0 ( col_inc , inc ) masuk_data_train_0 ( col_zip , zip1 ) masuk_data_train_0 ( col_fam , fam ) masuk_data_train_0 ( col_cca , cca ) masuk_data_train_0 ( col_edu , edu ) masuk_data_train_0 ( col_onl , online ) #memasukkan data train mempunyai nilai class 1 masuk_data_train_1 ( col_age , age ) masuk_data_train_1 ( col_exp , exp ) masuk_data_train_1 ( col_inc , inc ) masuk_data_train_1 ( col_zip , zip1 ) masuk_data_train_1 ( col_fam , fam ) masuk_data_train_1 ( col_cca , cca ) masuk_data_train_1 ( col_edu , edu ) masuk_data_train_1 ( col_onl , online ) #memasukkan data tes mempunyai nilai class 0 masuk_data_test_0 ( col_age , age_dt ) masuk_data_test_0 ( col_exp , exp_dt ) masuk_data_test_0 ( col_inc , inc_dt ) masuk_data_test_0 ( col_zip , zip1_dt ) masuk_data_test_0 ( col_fam , fam_dt ) masuk_data_test_0 ( col_cca , cca_dt ) masuk_data_test_0 ( col_edu , edu_dt ) masuk_data_test_0 ( col_onl , onl_dt ) #memasukkan data train mempunyai nilai class 1 masuk_data_test_1 ( col_age , age_dt ) masuk_data_test_1 ( col_exp , exp_dt ) masuk_data_test_1 ( col_inc , inc_dt ) masuk_data_test_1 ( col_zip , zip1_dt ) masuk_data_test_1 ( col_fam , fam_dt ) masuk_data_test_1 ( col_cca , cca_dt ) masuk_data_test_1 ( col_edu , edu_dt ) masuk_data_test_1 ( col_onl , onl_dt ) pcx ( age , exp , inc , zip1 , fam , cca , edu , online , age_dt , exp_dt , inc_dt , zip1_dt , fam_dt , cca_dt , edu_dt , onl_dt , knn , Hasil ) hasil ( onl_dt , Hasil , benar , data , accuracy ) df = pd . DataFrame ({ 'knn' : knn , 'databenar' : benar , 'Jmldata' : data , 'accuracy' : accuracy }) df . plot ( kind = 'bar' , x = 'knn' , y = 'accuracy' , color = 'blue' ) df . sort_values ( by = [ 'databenar' ], inplace = True , ascending = False ) print ( df ) plt . show ()","title":"Implementasi"},{"location":"K-Nearest Neighbor (KNN)/#referensi","text":"https://id.wikipedia.org/wiki/KNN https://www.advernesia.com/blog/data-science/pengertian-dan-cara-kerja-algoritma-k-nearest-neighbours-knn/ https://medium.com/bee-solution-partners/cara-kerja-algoritma-k-nearest-neighbor-k-nn-389297de543e http://cgeduntuksemua.blogspot.com/2012/03/pengertian-kelebihan-dan-kekurangan-k.html","title":"Referensi"}]}